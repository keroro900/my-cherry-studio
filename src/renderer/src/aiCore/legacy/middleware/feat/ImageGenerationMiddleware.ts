import { loggerService } from '@logger'
import { ModelRouter } from '@renderer/aiCore/routing/ModelRouter'
import { autonomousImageAgent, createBase64Ref, type ImageRef, intentAnalyzer } from '@renderer/pages/workflow/agents'
import { PromptService } from '@renderer/pages/workflow/prompts/PromptService'
import { WorkflowAiService } from '@renderer/pages/workflow/services/WorkflowAiService'
import FileManager from '@renderer/services/FileManager'
import {
  type ImageGenerationParams,
  ImageGenerationService,
  type StreamingGenerationOptions
} from '@renderer/services/ImageGenerationService'
import store from '@renderer/store'
import { type ImageAssistant, type ImageAssistantType, isImageAssistant } from '@renderer/types'
import { ChunkType } from '@renderer/types/chunk'
import { findImageBlocks, getMainTextContent } from '@renderer/utils/messageUtils/find'

import type { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
import type { CompletionsContext, CompletionsMiddleware } from '../types'

export const MIDDLEWARE_NAME = 'ImageGenerationMiddleware'
const logger = loggerService.withContext('ImageGenerationMiddleware')

// ==================== è‡ªä¸»æ¨¡å¼æ£€æµ‹ ====================

/**
 * è‡ªä¸»æ¨¡å¼è§¦å‘å…³é”®è¯
 * å½“ç”¨æˆ·æ¶ˆæ¯åŒ…å«è¿™äº›å…³é”®è¯æ—¶ï¼Œè§¦å‘è‡ªä¸»å›¾ç‰‡ç”Ÿæˆ
 */
const AUTONOMOUS_MODE_KEYWORDS = [
  // ä¸­æ–‡å…³é”®è¯
  'ä¸€æ•´å¥—',
  'å…¨å¥—',
  'æ•´å¥—',
  'å¸®æˆ‘ç”Ÿæˆ',
  'è‡ªä¸»ç”Ÿæˆ',
  'è‡ªåŠ¨ç”Ÿæˆ',
  'æ™ºèƒ½ç”Ÿæˆ',
  'ä¸€é”®ç”Ÿæˆ',
  // è‹±æ–‡å…³é”®è¯
  'full set',
  'complete set',
  'auto generate',
  'autonomous',
  'smart generate'
]

/**
 * æ£€æµ‹æ˜¯å¦åº”è¯¥ä½¿ç”¨è‡ªä¸»æ¨¡å¼
 */
function shouldUseAutonomousMode(userPrompt: string, hasImages: boolean): boolean {
  if (!hasImages) return false

  const lowerPrompt = userPrompt.toLowerCase()

  // æ£€æŸ¥æ˜¯å¦åŒ…å«è‡ªä¸»æ¨¡å¼å…³é”®è¯
  for (const keyword of AUTONOMOUS_MODE_KEYWORDS) {
    if (lowerPrompt.includes(keyword.toLowerCase())) {
      return true
    }
  }

  // ä½¿ç”¨ IntentAnalyzer æ£€æµ‹é«˜ç½®ä¿¡åº¦çš„ç”µå•†/æ¨¡ç‰¹ä»»åŠ¡
  const intent = intentAnalyzer.analyzeUserIntent(userPrompt)
  if (intent.confidence >= 0.6 && (intent.taskType === 'ecom' || intent.taskType === 'model')) {
    // å¦‚æœæ£€æµ‹åˆ°ç”µå•†æˆ–æ¨¡ç‰¹ä»»åŠ¡ä¸”æœ‰å›¾ç‰‡ï¼Œä¹Ÿå¯ä»¥è§¦å‘è‡ªä¸»æ¨¡å¼
    return true
  }

  return false
}

/**
 * è·å–å›¾ç‰‡å°ºå¯¸çš„åƒç´ å€¼
 */
function getImageSizeValue(imageSize: '1K' | '2K' | '4K'): string {
  const sizeMap: Record<string, string> = {
    '1K': '1024x1024',
    '2K': '2048x2048',
    '4K': '4096x4096'
  }
  return sizeMap[imageSize] || '1024x1024'
}

/**
 * å°† Uint8Array è½¬æ¢ä¸º base64 å­—ç¬¦ä¸²
 */
function uint8ArrayToBase64(uint8Array: Uint8Array): string {
  let binary = ''
  for (let i = 0; i < uint8Array.length; i++) {
    binary += String.fromCharCode(uint8Array[i])
  }
  return btoa(binary)
}

export const ImageGenerationMiddleware: CompletionsMiddleware =
  () =>
  (next) =>
  async (context: CompletionsContext, params: CompletionsParams): Promise<CompletionsResult> => {
    const { assistant, messages } = params
    const provider = context.apiClientInstance.provider
    const signal = context._internal?.flowControl?.abortSignal

    // ä½¿ç”¨ ModelRouter ç»Ÿä¸€åˆ¤æ–­æ˜¯å¦èµ°å›¾ç‰‡ç”Ÿæˆè·¯å¾„
    const isImageTypeAssistant = isImageAssistant(assistant)
    const shouldHandleImageGeneration = ModelRouter.shouldUseImageGeneration(assistant)

    // è°ƒè¯•æ—¥å¿—ï¼šè¿½è¸ªå›¾ç‰‡åŠ©æ‰‹é…ç½®
    logger.debug('ImageGenerationMiddleware - æ£€æŸ¥åŠ©æ‰‹ç±»å‹', {
      assistantId: assistant.id,
      assistantType: assistant.type,
      isImageTypeAssistant,
      shouldHandleImageGeneration,
      hasModel: !!assistant.model
    })

    if (!shouldHandleImageGeneration || typeof messages === 'string') {
      return next(context, params)
    }

    // è·å–å›¾ç‰‡åŠ©æ‰‹çš„é…ç½®
    const imageConfig = isImageTypeAssistant ? (assistant as ImageAssistant).imageConfig : null
    const imageModelId = isImageTypeAssistant ? (assistant as ImageAssistant).imageModelId : assistant.model?.id

    // è°ƒè¯•æ—¥å¿—ï¼šè¿½è¸ª imageConfig
    logger.debug('ImageGenerationMiddleware - å›¾ç‰‡åŠ©æ‰‹é…ç½®', {
      isImageTypeAssistant,
      hasImageConfig: !!imageConfig,
      imageSize: imageConfig?.imageSize,
      aspectRatio: imageConfig?.aspectRatio,
      batchCount: imageConfig?.batchCount,
      imageModelId
    })

    const stream = new ReadableStream<GenericChunk>({
      async start(controller) {
        const enqueue = (chunk: GenericChunk) => controller.enqueue(chunk)

        try {
          if (!imageModelId && !assistant.model) {
            throw new Error('Assistant model is not defined.')
          }

          const lastUserMessage = messages.findLast((m) => m.role === 'user')
          const lastAssistantMessage = messages.findLast((m) => m.role === 'assistant')

          if (!lastUserMessage) {
            throw new Error('No user message found for image generation.')
          }

          const userPrompt = getMainTextContent(lastUserMessage)

          // æ„å»ºå¢å¼ºçš„æç¤ºè¯ï¼ˆç»“åˆç³»ç»Ÿæç¤ºå’Œæ¨¡å—é…ç½®ï¼‰
          // ä½¿ç”¨ PromptService.buildForAssistant æ›¿ä»£åŸæœ‰çš„ buildEnhancedPrompt
          const imageAssistant = isImageTypeAssistant ? (assistant as ImageAssistant) : null
          const enhancedPrompt = isImageTypeAssistant
            ? PromptService.buildForAssistant({
                imageType: imageAssistant!.imageType as ImageAssistantType,
                moduleConfig: imageAssistant!.imageConfig?.moduleConfig,
                userPrompt,
                systemPrompt: imageAssistant!.prompt
              }).fullPrompt
            : userPrompt

          // æ”¶é›†å‚è€ƒå›¾ç‰‡ä¸º base64 æ ¼å¼ï¼ˆä¸å·¥ä½œæµä¿æŒä¸€è‡´ï¼‰
          const referenceImages: string[] = []

          // Collect images from user message
          const userImageBlocks = findImageBlocks(lastUserMessage)
          for (const block of userImageBlocks) {
            if (block.file) {
              const binaryData: Uint8Array = await FileManager.readBinaryImage(block.file)
              const base64 = uint8ArrayToBase64(binaryData)
              referenceImages.push(base64)
            } else if (block.url) {
              // å¦‚æœæ˜¯ data URLï¼Œæå– base64 éƒ¨åˆ†
              const base64Part = block.url.replace(/^data:image\/\w+;base64,/, '')
              if (base64Part !== block.url) {
                referenceImages.push(base64Part)
              }
            }
          }

          // Collect images from last assistant message
          if (lastAssistantMessage) {
            const assistantImageBlocks = findImageBlocks(lastAssistantMessage)
            for (const block of assistantImageBlocks) {
              if (block.url) {
                const base64Part = block.url.replace(/^data:image\/\w+;base64,/, '')
                if (base64Part !== block.url) {
                  referenceImages.push(base64Part)
                }
              }
            }
          }

          // ==================== è‡ªä¸»æ¨¡å¼æ£€æµ‹ä¸æ‰§è¡Œ ====================
          const useAutonomousMode = shouldUseAutonomousMode(userPrompt, referenceImages.length > 0)

          if (useAutonomousMode) {
            logger.info('ImageGenerationMiddleware - æ£€æµ‹åˆ°è‡ªä¸»æ¨¡å¼', {
              userPrompt: userPrompt.substring(0, 100),
              imageCount: referenceImages.length
            })

            enqueue({ type: ChunkType.IMAGE_CREATED })
            enqueue({
              type: ChunkType.THINKING_DELTA,
              text: 'ğŸ¤– å¯åŠ¨è‡ªä¸»å›¾ç‰‡ç”Ÿæˆæ¨¡å¼...\n\n'
            })

            try {
              // è·å– Provider é…ç½®
              const state = store.getState()
              const geminiProvider = state.llm.providers.find((p) => p.type === 'gemini' && p.apiKey)
              if (!geminiProvider) {
                throw new Error('è‡ªä¸»æ¨¡å¼éœ€è¦é…ç½® Gemini API Key')
              }
              const geminiModel = geminiProvider.models.find((m) => m.id.includes('flash') || m.id.includes('pro'))
              if (!geminiModel) {
                throw new Error('æœªæ‰¾åˆ°å¯ç”¨çš„ Gemini æ¨¡å‹')
              }

              // è®¾ç½®ç”Ÿæˆå’Œåˆ†æå‡½æ•°
              autonomousImageAgent.setGenerateImageFunc(async (params) => {
                try {
                  const imageResult = await WorkflowAiService.generateImage(geminiProvider, geminiModel, {
                    prompt: params.prompt,
                    systemPrompt: params.systemPrompt,
                    images: params.images,
                    aspectRatio: imageConfig?.aspectRatio || '3:4',
                    imageSize: imageConfig?.imageSize || '2K'
                  })
                  return { images: [imageResult] }
                } catch (error) {
                  return { images: [], error: error instanceof Error ? error.message : String(error) }
                }
              })

              autonomousImageAgent.setAnalyzeImageFunc(async (imgs, prompt) => {
                const loadedImages = await WorkflowAiService.loadImagesForVision(imgs)
                return WorkflowAiService.visionAnalysis(geminiProvider, geminiModel, {
                  systemPrompt:
                    'You are an expert fashion analyst. Analyze the image and provide structured information.',
                  userPrompt: prompt,
                  images: loadedImages
                })
              })

              // è½¬æ¢ä¸º ImageRef
              const imageRefs: ImageRef[] = referenceImages.map((base64) =>
                createBase64Ref(base64, { filename: 'input.png' })
              )

              // æ‰§è¡Œè‡ªä¸»ç”Ÿæˆ
              const startTime = Date.now()
              const result = await autonomousImageAgent.execute(
                {
                  userMessage: userPrompt,
                  images: imageRefs,
                  constraints: {
                    taskType: 'auto'
                  }
                },
                (progress) => {
                  // è¿›åº¦å›è°ƒ
                  enqueue({
                    type: ChunkType.THINKING_DELTA,
                    text: `${progress.message}\n`
                  })
                }
              )

              if (!result.success) {
                throw new Error(result.error || 'è‡ªä¸»ç”Ÿæˆå¤±è´¥')
              }

              // æ”¶é›†æ‰€æœ‰ç”Ÿæˆçš„å›¾ç‰‡
              const allImages: string[] = []
              const extractBase64Values = (refs?: ImageRef[]): string[] => {
                if (!refs) return []
                return refs.filter((img) => img.type === 'base64').map((img) => `data:image/png;base64,${img.value}`)
              }

              allImages.push(...extractBase64Values(result.images.main))
              allImages.push(...extractBase64Values(result.images.back))
              allImages.push(...extractBase64Values(result.images.detail))

              if (allImages.length === 0) {
                throw new Error('æœªç”Ÿæˆä»»ä½•å›¾ç‰‡')
              }

              // å‘é€ç»“æœ
              enqueue({
                type: ChunkType.THINKING_DELTA,
                text: `\nâœ… ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ ${allImages.length} å¼ å›¾ç‰‡\n`
              })

              enqueue({
                type: ChunkType.IMAGE_COMPLETE,
                image: { type: 'base64', images: allImages }
              })

              enqueue({
                type: ChunkType.LLM_RESPONSE_COMPLETE,
                response: {
                  usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 },
                  metrics: {
                    completion_tokens: 0,
                    time_first_token_millsec: 0,
                    time_completion_millsec: Date.now() - startTime
                  }
                }
              })

              controller.close()
              return
            } catch (error: any) {
              logger.error('è‡ªä¸»æ¨¡å¼æ‰§è¡Œå¤±è´¥', error)
              enqueue({
                type: ChunkType.TEXT_DELTA,
                text: `\nâŒ è‡ªä¸»æ¨¡å¼æ‰§è¡Œå¤±è´¥: ${error.message}\n\nåˆ‡æ¢åˆ°æ™®é€šç”Ÿæˆæ¨¡å¼...`
              })
              // ç»§ç»­æ‰§è¡Œæ™®é€šæ¨¡å¼
            }
          }

          // ==================== æ™®é€šå›¾ç‰‡ç”Ÿæˆæ¨¡å¼ ====================
          enqueue({ type: ChunkType.IMAGE_CREATED })

          const startTime = Date.now()
          const modelId = imageModelId || assistant.model?.id || ''
          const model = assistant.model || { id: modelId, name: modelId, provider: provider.id, group: 'image' }

          // æ„å»ºå›¾ç‰‡ç”Ÿæˆå‚æ•°ï¼ˆä¸å·¥ä½œæµä¿æŒä¸€è‡´ï¼‰
          // å…³é”®ï¼šåŒæ—¶ä¼ é€’ image_sizeï¼ˆåŸå§‹å€¼å¦‚ "2K"ï¼‰å’Œ sizeï¼ˆåƒç´ å€¼å¦‚ "2048x2048"ï¼‰
          const serviceParams: ImageGenerationParams = {
            model: modelId,
            prompt: enhancedPrompt || '',
            n: 1,
            response_format: 'b64_json'
          }

          // åº”ç”¨å›¾ç‰‡åŠ©æ‰‹é…ç½®
          if (imageConfig) {
            // è®¾ç½®å›¾ç‰‡å°ºå¯¸ - å…³é”®ï¼šåŒæ—¶ä¼ é€’ image_size å’Œ size
            if (imageConfig.imageSize) {
              serviceParams.image_size = imageConfig.imageSize // åŸå§‹å€¼å¦‚ "2K"
              serviceParams.size = getImageSizeValue(imageConfig.imageSize) // åƒç´ å€¼å¦‚ "2048x2048"
            }
            // è®¾ç½®å®½é«˜æ¯”ï¼ˆç›´æ¥ä¼ é€’ï¼Œå¦‚ "1:1", "3:4", "16:9"ï¼‰
            if (imageConfig.aspectRatio) {
              serviceParams.aspect_ratio = imageConfig.aspectRatio
            }
            // è®¾ç½®æ‰¹é‡ç”Ÿæˆæ•°é‡
            if (imageConfig.batchCount && imageConfig.batchCount > 1) {
              serviceParams.n = imageConfig.batchCount
            }

            // è°ƒè¯•æ—¥å¿—ï¼šè¿½è¸ªå®é™…ä¼ é€’çš„å‚æ•°
            logger.debug('ImageGenerationMiddleware - åº”ç”¨ imageConfig åˆ° serviceParams', {
              image_size: serviceParams.image_size,
              size: serviceParams.size,
              aspect_ratio: serviceParams.aspect_ratio,
              n: serviceParams.n
            })
          }

          // æ·»åŠ å‚è€ƒå›¾ç‰‡
          if (referenceImages.length > 0) {
            serviceParams.image = referenceImages[0]
            serviceParams.reference_images = referenceImages
          }

          // æ£€æŸ¥ Azure OpenAI GPT-Image-1-Mini é™åˆ¶
          if (
            referenceImages.length > 0 &&
            model.id.toLowerCase().includes('gpt-image-1-mini') &&
            provider.type === 'azure-openai'
          ) {
            throw new Error('Azure OpenAI GPT-Image-1-Mini model does not support image editing.')
          }

          // ä½¿ç”¨ ImageGenerationService è¿›è¡Œæµå¼å›¾ç‰‡ç”Ÿæˆ
          // æµå¼ç”Ÿæˆæ”¯æŒæ˜¾ç¤ºæ€è€ƒæ­¥éª¤
          const imageService = new ImageGenerationService(provider, model)

          // æ”¶é›†ç”Ÿæˆçš„å›¾ç‰‡
          const generatedImages: string[] = []

          // æµå¼ç”Ÿæˆé€‰é¡¹
          const streamingOptions: StreamingGenerationOptions = {
            signal,
            // æ–‡æœ¬/æ€è€ƒå†…å®¹å›è°ƒ
            onTextChunk: (text, type) => {
              if (type === 'thinking') {
                // æ€è€ƒå†…å®¹
                enqueue({
                  type: ChunkType.THINKING_DELTA,
                  text
                })
              } else {
                // æ™®é€šæ–‡æœ¬å†…å®¹
                enqueue({
                  type: ChunkType.TEXT_DELTA,
                  text
                })
              }
            },
            // å›¾ç‰‡å›è°ƒ
            onImageChunk: (image) => {
              generatedImages.push(image)
            }
          }

          // ä½¿ç”¨æµå¼ç”Ÿæˆ
          const result = await imageService.generateStreaming(serviceParams, streamingOptions)

          if (!result.success || result.images.length === 0) {
            throw new Error(result.error || 'å›¾ç‰‡ç”Ÿæˆå¤±è´¥')
          }

          // ç¡®å®šå›¾ç‰‡ç±»å‹
          let imageType: 'url' | 'base64' = 'base64'
          const imageList = result.images.map((img) => {
            if (img.startsWith('http://') || img.startsWith('https://')) {
              imageType = 'url'
              return img
            }
            // ç¡®ä¿è¿”å›å®Œæ•´çš„ data URL
            if (img.startsWith('data:')) {
              return img
            }
            return `data:image/png;base64,${img}`
          })

          enqueue({
            type: ChunkType.IMAGE_COMPLETE,
            image: { type: imageType, images: imageList }
          })

          enqueue({
            type: ChunkType.LLM_RESPONSE_COMPLETE,
            response: {
              usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 },
              metrics: {
                completion_tokens: 0,
                time_first_token_millsec: 0,
                time_completion_millsec: Date.now() - startTime
              }
            }
          })
        } catch (error: any) {
          enqueue({ type: ChunkType.ERROR, error })
        } finally {
          controller.close()
        }
      }
    })

    return {
      stream,
      getText: () => ''
    }
  }
