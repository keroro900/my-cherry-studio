/**
 * Native VCP æ¨¡å—åŠŸèƒ½æµ‹è¯•
 *
 * æµ‹è¯•ä» VCP rust-vexus-lite è¿ç§»çš„æ¨¡å—:
 * - VexusIndex (HNSW å‘é‡ç´¢å¼•)
 * - CooccurrenceMatrix (NPMI å…±ç°çŸ©é˜µ)
 * - SemanticGroupMatcher (è¯­ä¹‰ç»„åŒ¹é…)
 * - ChineseSearchEngine (ä¸­æ–‡å…¨æ–‡æœç´¢)
 */

const path = require('path')
const fs = require('fs')

// åŠ è½½åŸç”Ÿæ¨¡å—
let native
try {
  native = require('./index.js')
  console.log('âœ… Native module loaded successfully')
  console.log('   Version:', native.getVersion())
  console.log('   Health:', JSON.stringify(native.healthCheck(), null, 2))
} catch (error) {
  console.error('âŒ Failed to load native module:', error.message)
  process.exit(1)
}

// æµ‹è¯•ä¸´æ—¶ç›®å½•
const TEST_DIR = path.join(__dirname, '.test-temp')
if (!fs.existsSync(TEST_DIR)) {
  fs.mkdirSync(TEST_DIR, { recursive: true })
}

console.log('\n' + '='.repeat(60))
console.log('ğŸ“Š Testing VexusIndex (HNSW Vector Index)')
console.log('='.repeat(60))

try {
  // åˆ›å»º 1536 ç»´å‘é‡ç´¢å¼• (OpenAI embedding ç»´åº¦)
  const vexus = new native.VexusIndex(1536, 1000)
  console.log('âœ… VexusIndex created (1536 dim, 1000 capacity)')

  // åˆ›å»ºæµ‹è¯•å‘é‡
  const createVector = (dim, seed = 0) => {
    const buffer = Buffer.alloc(dim * 4) // Float32 = 4 bytes
    for (let i = 0; i < dim; i++) {
      buffer.writeFloatLE(Math.sin(seed + i * 0.1), i * 4)
    }
    return buffer
  }

  // æ·»åŠ å‘é‡
  vexus.add(1, createVector(1536, 0))
  vexus.add(2, createVector(1536, 1))
  vexus.add(3, createVector(1536, 2))
  console.log('âœ… Added 3 vectors')

  // æ‰¹é‡æ·»åŠ 
  const batchIds = [4, 5, 6]
  const batchVectors = Buffer.alloc(3 * 1536 * 4)
  for (let i = 0; i < 3; i++) {
    const vec = createVector(1536, i + 3)
    vec.copy(batchVectors, i * 1536 * 4)
  }
  vexus.addBatch(batchIds, batchVectors)
  console.log('âœ… Batch added 3 more vectors')

  // æœç´¢
  const queryVec = createVector(1536, 0) // åº”è¯¥æœ€åŒ¹é… id=1
  const results = vexus.search(queryVec, 3)
  console.log('âœ… Search results:', results)

  // ç»Ÿè®¡
  const stats = vexus.stats()
  console.log('âœ… Stats:', stats)

  // ä¿å­˜
  const indexPath = path.join(TEST_DIR, 'test-vexus.usearch')
  vexus.save(indexPath)
  console.log('âœ… Saved to:', indexPath)

  // é‡æ–°åŠ è½½
  const loaded = native.VexusIndex.load(indexPath, 1536, 1000)
  console.log('âœ… Loaded from disk, size:', loaded.size())

} catch (error) {
  console.error('âŒ VexusIndex test failed:', error.message)
}

console.log('\n' + '='.repeat(60))
console.log('ğŸ“Š Testing CooccurrenceMatrix (NPMI Tag Cooccurrence)')
console.log('='.repeat(60))

try {
  const matrix = new native.CooccurrenceMatrix()
  console.log('âœ… CooccurrenceMatrix created')

  // ä»æ–‡æ¡£æ„å»º
  const documents = [
    { id: 'doc1', tags: ['çº¢è‰²', 'æ­£å¼', 'è¥¿è£…'] },
    { id: 'doc2', tags: ['è“è‰²', 'ä¼‘é—²', 'Tæ¤'] },
    { id: 'doc3', tags: ['çº¢è‰²', 'å•†åŠ¡', 'è¡¬è¡«'] },
    { id: 'doc4', tags: ['çº¢è‰²', 'æ­£å¼', 'è£™å­'] },
    { id: 'doc5', tags: ['è“è‰²', 'ä¼‘é—²', 'ç‰›ä»”'] },
    { id: 'doc6', tags: ['çº¢è‰²', 'å•†åŠ¡', 'è¥¿è£…'] },
  ]

  const pairCount = matrix.buildFromDocuments(documents)
  console.log('âœ… Built from documents, pair count:', pairCount)

  // æŸ¥è¯¢å…±ç°æƒé‡
  const weight = matrix.getCooccurrence('çº¢è‰²', 'æ­£å¼')
  console.log('âœ… Cooccurrence(çº¢è‰², æ­£å¼):', weight.toFixed(4))

  // è·å–ç›¸å…³æ ‡ç­¾
  const related = matrix.getRelatedTags('çº¢è‰²', 5, 0.1)
  console.log('âœ… Related to çº¢è‰²:', related.map(r => `${r.tag2}(${r.weight.toFixed(3)})`).join(', '))

  // å¤šè·³æ‰©å±•
  const expanded = matrix.expandTags(['çº¢è‰²', 'è¥¿è£…'], 2, 0.7)
  console.log('âœ… Expanded from [çº¢è‰², è¥¿è£…]:', expanded.length, 'tags')

  // è®¡ç®— boost
  const boost = matrix.calculateBoost(['çº¢è‰²', 'è¥¿è£…'], ['æ­£å¼', 'å•†åŠ¡'], 0.3, 0.1)
  console.log('âœ… Boost score:', boost.toFixed(4))

  // åºåˆ—åŒ–
  const json = matrix.toJson()
  const restored = native.CooccurrenceMatrix.fromJson(json)
  console.log('âœ… Serialization/deserialization OK, tag count:', restored.tagCount())

} catch (error) {
  console.error('âŒ CooccurrenceMatrix test failed:', error.message)
}

console.log('\n' + '='.repeat(60))
console.log('ğŸ“Š Testing SemanticGroupMatcher')
console.log('='.repeat(60))

try {
  // ä½¿ç”¨é»˜è®¤æœè£…è¯­ä¹‰ç»„
  const matcher = native.SemanticGroupMatcher.withFashionGroups()
  console.log('âœ… SemanticGroupMatcher created with fashion groups')
  console.log('   Keyword count:', matcher.keywordCount())
  console.log('   Group types:', matcher.getGroupTypes().join(', '))

  // æå–åŒ¹é…
  const text = 'æˆ‘æƒ³è¦ä¸€ä»¶çº¢è‰²ä¼‘é—²çš„çº¯æ£‰Tæ¤ï¼Œé€‚åˆæ˜¥å¤ç©¿'
  const matches = matcher.extractMatches(text)
  console.log('âœ… Extracted matches from:', text)
  for (const m of matches) {
    console.log(`   - ${m.groupType}/${m.subGroup}: [${m.matchedKeywords.join(', ')}] (weight: ${m.weight.toFixed(2)})`)
  }

  // æ‰©å±•å…³é”®è¯
  const expanded = matcher.expandKeywords(matches)
  console.log('âœ… Expanded keywords:', expanded.slice(0, 5).join(', '), '...')

  // è‡ªå®šä¹‰ç»„
  const custom = new native.SemanticGroupMatcher()
  custom.registerGroup('brand', 'luxury', ['LV', 'Gucci', 'Prada', 'Chanel'])
  custom.registerGroup('brand', 'sports', ['Nike', 'Adidas', 'Puma'])
  console.log('âœ… Custom matcher with', custom.keywordCount(), 'keywords')

} catch (error) {
  console.error('âŒ SemanticGroupMatcher test failed:', error.message)
}

console.log('\n' + '='.repeat(60))
console.log('ğŸ“Š Testing ChineseSearchEngine (jieba + tantivy)')
console.log('='.repeat(60))

try {
  const searchPath = path.join(TEST_DIR, 'chinese-search-index')
  const engine = native.ChineseSearchEngine.open(searchPath)
  console.log('âœ… ChineseSearchEngine opened at:', searchPath)

  // æ·»åŠ æ–‡æ¡£
  const docs = [
    { id: 'doc1', title: 'äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿', content: 'æ·±åº¦å­¦ä¹ å’Œå¤§è¯­è¨€æ¨¡å‹æ­£åœ¨æ”¹å˜å„ä¸ªè¡Œä¸šï¼Œæœºå™¨å­¦ä¹ æŠ€æœ¯æ—¥æ–°æœˆå¼‚', tags: ['AI', 'æ·±åº¦å­¦ä¹ '] },
    { id: 'doc2', title: 'Python ç¼–ç¨‹å…¥é—¨', content: 'Python æ˜¯ä¸€é—¨ç®€å•æ˜“å­¦çš„ç¼–ç¨‹è¯­è¨€ï¼Œé€‚åˆåˆå­¦è€…å…¥é—¨', tags: ['ç¼–ç¨‹', 'Python'] },
    { id: 'doc3', title: 'æœºå™¨å­¦ä¹ å®æˆ˜', content: 'æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ', tags: ['æœºå™¨å­¦ä¹ ', 'AI'] },
  ]

  const added = engine.addDocuments(docs)
  engine.commit()
  console.log('âœ… Added', added, 'documents')

  // æœç´¢
  const results = engine.search('æœºå™¨å­¦ä¹ ', 10)
  console.log('âœ… Search results for "æœºå™¨å­¦ä¹ ":')
  for (const r of results) {
    console.log(`   - ${r.id}: ${r.title} (score: ${r.score.toFixed(2)})`)
  }

  // åˆ†è¯
  const tokens = engine.tokenize('æˆ‘æ¥è‡ªåŒ—äº¬æ¸…åå¤§å­¦è®¡ç®—æœºç³»')
  console.log('âœ… Tokenize result:', tokens.join(' | '))

  // å…³é”®è¯æå–
  const keywords = engine.extractKeywords('æ·±åº¦å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæ­£åœ¨æ”¹å˜å„ä¸ªè¡Œä¸š', 5)
  console.log('âœ… Keywords:', keywords.map(k => `${k.keyword}(${k.weight})`).join(', '))

  // ç»Ÿè®¡
  const stats = engine.getStats()
  console.log('âœ… Document count:', stats.documentCount)

} catch (error) {
  console.error('âŒ ChineseSearchEngine test failed:', error.message)
}

console.log('\n' + '='.repeat(60))
console.log('ğŸ“Š Testing Standalone jieba Functions')
console.log('='.repeat(60))

try {
  // jiebaCut
  const tokens = native.jiebaCut('æˆ‘å–œæ¬¢åœ¨åŒ—äº¬æ¸…åå¤§å­¦è¯»ä¹¦', true)
  console.log('âœ… jiebaCut:', tokens.join(' | '))

  // jiebaExtractKeywords
  const keywords = native.jiebaExtractKeywords('äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æ˜¯å½“ä»Šæœ€çƒ­é—¨çš„æŠ€æœ¯é¢†åŸŸ', 5)
  console.log('âœ… jiebaExtractKeywords:', keywords.map(k => k.keyword).join(', '))

} catch (error) {
  console.error('âŒ jieba functions test failed:', error.message)
}

console.log('\n' + '='.repeat(60))
console.log('ğŸ“Š Testing Vector Operations')
console.log('='.repeat(60))

try {
  const a = [1, 2, 3, 4, 5]
  const b = [5, 4, 3, 2, 1]

  const cosine = native.cosineSimilarity(a, b)
  console.log('âœ… Cosine similarity:', cosine.toFixed(4))

  const euclidean = native.euclideanDistance(a, b)
  console.log('âœ… Euclidean distance:', euclidean.toFixed(4))

  const dot = native.dotProduct(a, b)
  console.log('âœ… Dot product:', dot)

  const normalized = native.normalize(a)
  console.log('âœ… Normalized:', normalized.map(v => v.toFixed(3)).join(', '))

  // æ‰¹é‡è®¡ç®—
  const query = [1, 0, 0, 0, 0]
  const vectors = [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0.5, 0.5, 0, 0, 0]]
  const similarities = native.batchCosineSimilarity(query, vectors)
  console.log('âœ… Batch cosine:', similarities.map(s => s.toFixed(3)).join(', '))

  const topK = native.topKSimilar(query, vectors, 2)
  console.log('âœ… Top-K:', topK.map(r => `idx=${r.index} score=${r.score.toFixed(3)}`).join(', '))

} catch (error) {
  console.error('âŒ Vector operations test failed:', error.message)
}

// æ¸…ç†
console.log('\n' + '='.repeat(60))
console.log('ğŸ§¹ Cleanup')
console.log('='.repeat(60))

try {
  fs.rmSync(TEST_DIR, { recursive: true, force: true })
  console.log('âœ… Cleaned up test directory')
} catch (error) {
  console.log('âš ï¸ Cleanup warning:', error.message)
}

console.log('\nâœ… All tests completed!')
